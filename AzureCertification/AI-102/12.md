# Develop Generative AI solutions with Azure OpenAI Service

- You can create two Azure OpenAI resources per region.
- Azure OpenAI Studio provides access to model management, deployment, experimentation, customization, and learning resources.
- Azure OpenAI includes several types of model:
    - GPT-4 models
    - GPT-3.5 models
    - Embeddings models
    - DALL-E models
- GPT-35-turbo models are optimized for chat-based interactions and work well in most generative AI scenarios.
- Pricing is determined by tokens and by model type.
- Several factors affect the quality of completions you'll get from a generative AI solution.
    - The way a prompt is engineered. 
    - The model parameters (covered next)
    - The data the model is trained on, which can be adapted through model fine-tuning with customization
- The Completions playground allows you to make calls to your deployed models through a text-in, text-out interface and to adjust parameters. 
- There are many parameters in compeletions playground that you can adjust to change the performance of your model:
    - Temperature
    - Max length
    - Stop sequences
    - Top probabilities
    - Frequency penalty
    - Presence penalty
    - Pre-response text
    - Post-response text
- Try adjusting temperature or Top P **but not both.**
- In the Chat playground, you're able to add **few-shot examples**. The term few-shot refers to providing a few of examples to help the model learn what it needs to do.
- `Completion` is available for all gpt-3 generation models, while `ChatCompletion` is the only supported option for `gpt-4` models and is the preferred endpoint when using the `gpt-35-turbo` model.
- If harmful content is included in the prompt, the API request returns an error.
- Each call to the Azure OpenAI service requires a key, endpoint, and deployment name.
- Improving prompt quality through various techniques is called **prompt engineering**.
- Prompt engineering can also help mitigate bias and improve fairness in AI models. 
- Functionally, `ChatCompletion` has the option of defining a `system message` for the AI model, in addition to built-in structure to provide previous messages in the prompt. 
- If using `Completion`, this functionality can be achieved with what's called a `meta prompt`. 
- A specific technique for formatting instructions is to split the instructions at the beginning or end of the prompt, and have the user content contained within --- or ### blocks. These tags allow the model to more clearly differentiate between instructions and content.
- **Primary content** refers to content that is the subject of the query, such a sentence to translate or an article to summarize.
- **Supporting content** is content that may alter the response, but isn't the focus or subject of the prompt.
- **Grounding content** allows the model to provide reliable answers by providing content for the model to draw answer from.
- Grounding content differs from primary content as it's the source of information to answer the prompt query, instead of the content being operated on for things like summarization or translation. 
- The system message is included at the beginning of a prompt and is designed to give the model instructions, perspective to answer from, or other information helpful to guide the model's response.
- Conversation history enables the model to continue responding in a similar way (such as tone or formatting) and allow the user to reference previous content in subsequent queries.
- AI models can be used for coding as:
    - Write function
    - change coding language
    - Understand unknown code
    - Complete partial code
    - Write unit tests
    - Add comments and generate documentation
    - Fix bugs in your code
    - Improve performance
    - Refactor inefficient code
- The REST API request for DALL-E must contain the following parameters in a JSON body:
    - prompt: The description of the image to be generated.
    - n: The number of images to be generated.
    - size: The resolution of the image(s) to be generated (256x256, 512x512, or 1024x1024).
-  **Azure OpenAI on your data** allows developers to use supported AI chat models that can reference specific sources of data to ground the response.
- Using Azure AI Studio allows the appropriate chunking to happen when inserting into the index, yielding better responses.
- Breaking down the task and using chain of thought prompting can help the model respond more effectively within the token limit.
- The Microsoft guidance for responsible generative AI is designed to be practical and actionable. It defines a four stage process to develop and implement a plan for responsible AI when using generative models. The **four stages** in the process are:
    - **Identify** potential harms that are relevant to your planned solution.
    - **Measure** the presence of these harms in the outputs generated by your solution.
    - **Mitigate** the harms at multiple layers in your solution to minimize their presence and impact, and ensure transparent communication about potential risks to users.
    - **Operate** the solution responsibly by defining and following a deployment and operational readiness plan.
- Four steps in identifiying potential harm:
    - Identify potential harms
    - Prioritize identified harms
    - Test and verify the prioritized harms
    - Document and share the verified harms
- Mitigation of potential harms in a generative AI solution involves a layered approach, in which mitigation techniques can be applied at each of four layers, as shown here:
    - Model
    - Safety System
    - Application
    - Positioning
- Common compliance reviews include:
    - Legal
    - Privacy
    - Security
    - Accessibility
- Content filters enable you to suppress harmful content at the Safety System layer.
- An initial release to a restricted user base enables you to minimize harm by gather feedback and identifying issues before broad release.