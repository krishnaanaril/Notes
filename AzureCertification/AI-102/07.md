# Build custom text analytics solutions

- Custom text classification falls into two types of projects:
    - Single label classification
    - Multiple label classification
- Language service project life cycle for text classification:
    - Define labels
    - Tag data
    - Train model
    - View model
    - Improve model
    - Deploy model
    - Classify text
- The training dataset will be the larger of the two datasets, recommended to be about 80% of your labeled data.
- During the Train model step, there are two options for how to train your model.
    - **Automatic** split - Azure takes all of your data, splits it into the specified percentages randomly, and applies them in training the model. This option is best when you have a larger dataset, data is naturally more consistent, or the distribution of your data extensively covers your classes.
    - **Manual** split - Manually specify which files should be in each dataset. When you submit the training job, the Language service will tell you the split of the dataset and the distribution. This split is best used with smaller datasets to ensure the correct distribution of classes and variation in data are present to correctly train your model.
- The API for the Language service operates asynchronously for most calls. In each step we submit a request to the service first, then check back with the service via a subsequent call to get the status or result.
- **Custom named entity recognition (NER)**, otherwise known as custom entity extraction, is one of the many natural language processing (NLP) features offered by Azure Language service. Custom NER enables developers to extract pre-defined entities from text documents, without those documents being in a known format - such as legal agreements or online ads.
- Language service project life cycle for NER:
    - Define entities
    - Tag data
    - Train model
    - view model
    - Imporve model
    - Deploy model
    - Extract entities
- Ideally we want our model to score well in both precision and recall, which means the entity recognition works well. If both metrics have a low score, it means the model is both struggling to recognize entities in the document, and when it does extract that entity, it doesn't assign it the correct label with high confidence.
- If precision is low but recall is high, it means that the model recognizes the entity well but doesn't label it as the correct entity type.
- If precision is high but recall is low, it means that the model doesn't always recognize the entity, but when the model extracts the entity, the correct label is applied.