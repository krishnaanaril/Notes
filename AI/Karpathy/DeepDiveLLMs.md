# [Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI)

- Fineweb dataset and blogpost
- Byte pair encoding algorithm
- GPT 4 use approx. 10^5 symbols.
- LLM Visualizer: https://bbycroft.net/llm
- https://github.com/karpathy/llm.c/discussions/677
- GPT - Generative Pretrained Transformer
- GPT 2:
    - Transformer neural network with:
        - 1.6 billion parameters
        - 1024 tokens maximum context length
- `im_start` means imaginary monologue start
- [The Llama 3 Herd of Models](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)
- Vague recollection vs working memory
    - Knowledge in the paramters == Vague recolleciton (e.g of something you read 1 month ago)
    - Knowledge in the tokens of the context window == working memory
- SFT - Supervised Fine Tuning